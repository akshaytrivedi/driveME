{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from generate_proposals import GenerateProposals\n",
    "from roi_align import RoIAlignFunction, preprocess_rois\n",
    "import utils.vis as vis_utils\n",
    "import utils.result_utils as result_utils\n",
    "import skimage.io as io\n",
    "from utils.blob import prep_im_for_blob\n",
    "import utils.dummy_datasets as dummy_datasets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_101_FPN_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/35861858-R-101-FPN/model_final.pkl\"\n",
    "X_152_32x8d_FPN_IN5k_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/37129812-X-152-32x8d-FPN-IN5k/model_final.pkl\"\n",
    "R_101_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/R-101/R-101.pkl\"\n",
    "X_152_32x8d_IN5k_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/X-152-32x8d-IN5k/X-152-32x8d-IN5k.pkl\"\n",
    "\n",
    "R_50_C4_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/tmp/model_final.pkl\"\n",
    "R_50_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/tmp/R-50.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self, backbone_architecture, pretrained_model_file, resnet_feature_extraction_layers=['conv1','bn1','relu','maxpool','layer1','layer2','layer3']):\n",
    "        super(ResnetModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.resnet_model = eval('models.' + backbone_architecture + '()') # construct ResNet model (maybe not very safe :) \n",
    "\n",
    "        # swap stride (2,2) and (1,1) in first layers (PyTorch ResNet is slightly different to caffe2 ResNet)\n",
    "        # this is required for compatibility with caffe2 models\n",
    "        self.resnet_model.layer2[0].conv1.stride=(2,2)\n",
    "        self.resnet_model.layer2[0].conv2.stride=(1,1)\n",
    "        self.resnet_model.layer3[0].conv1.stride=(2,2)\n",
    "        self.resnet_model.layer3[0].conv2.stride=(1,1)\n",
    "        self.resnet_model.layer4[0].conv1.stride=(2,2)\n",
    "        self.resnet_model.layer4[0].conv2.stride=(1,1)\n",
    "        \n",
    "        self.init_weights(pretrained_model_file)\n",
    "        \n",
    "        # All except the last layer are used as feature extractor... Last layer is for ROI pooling\n",
    "        self.model = torch.nn.Sequential(*[getattr(self.resnet_model, layer) for layer in resnet_feature_extraction_layers])\n",
    "        self.model.eval()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.model(image)\n",
    "    \n",
    "    def init_weights(self, pretrained_model_file):\n",
    "        with open(pretrained_model_file, 'rb') as model_pickle_file:\n",
    "            fb_model = pickle.load(model_pickle_file)\n",
    "            # Model has two keys- config and blobs\n",
    "            fb_model = fb_model['blobs']\n",
    "        \n",
    "        model_dict = self.resnet_model.state_dict()\n",
    "        \n",
    "        for key in model_dict.keys():\n",
    "            # skip running mean/std and fc weights\n",
    "            # I am not sure what running is but fc is the last fuly connected layer of resnet.. so fb model doesnt have it\n",
    "            if 'running' in key or 'fc' in key:\n",
    "                continue\n",
    "            \n",
    "            fb_key = self.convert_key_to_fb_format(key.split('.'))\n",
    "           \n",
    "            assert model_dict[key].size()==torch.FloatTensor(fb_model[fb_key]).size()\n",
    "            \n",
    "            if key=='conv1.weight': # convert from BGR to RGB                \n",
    "                model_dict[key]=torch.FloatTensor(fb_model[fb_key][:,(2, 1, 0),:,:])\n",
    "            else:\n",
    "                model_dict[key]=torch.FloatTensor(fb_model[fb_key])\n",
    "        \n",
    "        # update model\n",
    "        self.resnet_model.load_state_dict(model_dict)\n",
    "\n",
    "    def convert_key_to_fb_format(self, terms, i=0, parsed=''):\n",
    "        # Convert PyTorch ResNet weight names to caffe2 weight names\n",
    "        if i==0:\n",
    "            if terms[i]=='conv1':\n",
    "                parsed='conv1'\n",
    "            elif terms[i]=='bn1':\n",
    "                parsed='res_conv1'\n",
    "            elif terms[i].startswith('layer'):\n",
    "                parsed='res'+str(int(terms[i][-1])+1)\n",
    "        else:\n",
    "            if terms[i]=='weight' and (terms[i-1].startswith('conv') or terms[i-1]=='0'):\n",
    "                parsed+='_w'\n",
    "            elif terms[i]=='weight' and (terms[i-1].startswith('bn') or terms[i-1]=='1'):\n",
    "                parsed+='_bn_s'\n",
    "            elif terms[i]=='bias' and (terms[i-1].startswith('bn') or terms[i-1]=='1'):\n",
    "                parsed+='_bn_b'\n",
    "            elif terms[i-1].startswith('layer'):\n",
    "                parsed+='_'+terms[i]\n",
    "            elif terms[i].startswith('conv') or terms[i].startswith('bn'):\n",
    "                parsed+='_branch2'+chr(96+int(terms[i][-1]))\n",
    "            elif terms[i]=='downsample':\n",
    "                parsed+='_branch1'\n",
    "        # increase counter\n",
    "        i+=1\n",
    "        # do recursion\n",
    "        if i==len(terms):\n",
    "            return parsed\n",
    "        return self.convert_key_to_fb_format(terms,i,parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, pretrained_model_file, feature_extractor_output_channels, rpn_conv_output_channels, number_of_anchors):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        \n",
    "        #RPN is used propose regions with probability of foreground/background.. i.e just tell if object is present\n",
    "        # It has 3 parts:\n",
    "        # 1) 3x3 conv with 512/1024 channels\n",
    "        # 2) 1x1 conv with 2k channels (for each anchor box we predict foreground/background)\n",
    "        # 3) 1x1 conv with 4k channels (for each anchor box we predict delta of boxes)\n",
    "        \n",
    "        self.conv_rpn = torch.nn.Conv2d(in_channels=feature_extractor_output_channels,\n",
    "                                        out_channels=rpn_conv_output_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "        self.rpn_cls_prob = torch.nn.Conv2d(in_channels=rpn_conv_output_channels,\n",
    "                                            out_channels=number_of_anchors,\n",
    "                                            kernel_size=1,\n",
    "                                            stride=1,\n",
    "                                            padding=0)\n",
    "        self.rpn_bbox_pred = torch.nn.Conv2d(in_channels=rpn_conv_output_channels,\n",
    "                                             out_channels=4*number_of_anchors,\n",
    "                                             kernel_size=1,\n",
    "                                             stride=1,\n",
    "                                             padding=0)\n",
    "        \n",
    "        self.init_weights(pretrained_model_file)\n",
    "        \n",
    "    def forward(self, anchor_features):\n",
    "        # image features shape should be (N,Cin,H,W)  \n",
    "        conv_anchor_features = F.relu(self.conv_rpn(anchor_features))\n",
    "        anchor_cls_prob =  F.softmax(self.rpn_cls_prob(conv_anchor_features))\n",
    "        anchor_box_pred =  self.rpn_bbox_pred(conv_anchor_features)\n",
    "        return anchor_cls_prob, anchor_box_pred\n",
    "\n",
    "    def init_weights(self, pretrained_model_file):\n",
    "        with open(pretrained_model_file, 'rb') as model_pickle_file:\n",
    "            fb_model = pickle.load(model_pickle_file)\n",
    "            # Model has two keys- config and blobs\n",
    "            fb_model = fb_model['blobs']\n",
    "        \n",
    "            self.conv_rpn.weight.data = torch.FloatTensor(fb_model['conv_rpn_w'])\n",
    "            self.conv_rpn.bias.data = torch.FloatTensor(fb_model['conv_rpn_b'])\n",
    "            self.rpn_cls_prob.weight.data = torch.FloatTensor(fb_model['rpn_cls_logits_w'])\n",
    "            self.rpn_cls_prob.bias.data = torch.FloatTensor(fb_model['rpn_cls_logits_b'])\n",
    "            self.rpn_bbox_pred.weight.data = torch.FloatTensor(fb_model['rpn_bbox_pred_w'])\n",
    "            self.rpn_bbox_pred.bias.data = torch.FloatTensor(fb_model['rpn_bbox_pred_b'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ROI_POOLING\n",
    "class ROI_Pooling(nn.Module):\n",
    "    def __init__(self, roi_height, roi_width, roi_spatial_scale, roi_sampling_ratio, conv_head_layers, resnet_model):\n",
    "        super(ROI_Pooling, self).__init__()\n",
    "        self.roi_height = roi_height\n",
    "        self.roi_width  = roi_width\n",
    "        self.roi_spatial_scale = roi_spatial_scale\n",
    "        self.roi_sampling_ratio = roi_sampling_ratio\n",
    "        self.conv_head = nn.Sequential(*[getattr(resnet_model, layer) for layer in conv_head_layers]) \n",
    "        \n",
    "    def forward(self, img_features, rois):\n",
    "        roi_features = RoIAlignFunction.apply(img_features, preprocess_rois(rois), self.roi_height, self.roi_width, self.roi_spatial_scale, self.roi_sampling_ratio)\n",
    "        \n",
    "        # compute 1x1 roi features\n",
    "        roi_features = self.conv_head(roi_features) # 1x1 feature per proposal\n",
    "        roi_features = roi_features.view(roi_features.size(0),-1)\n",
    "        \n",
    "        return roi_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, pretrained_model_file, roi_feature_size, N_classes):\n",
    "        super(RCNN, self).__init__()\n",
    "        # What will be the size of roi_feature_channels??\n",
    "        self.bbox_head=torch.nn.Linear(roi_feature_size, 4*N_classes)\n",
    "        self.class_prob_head=torch.nn.Linear(roi_feature_size, N_classes)\n",
    "        \n",
    "        self.init_weights(pretrained_model_file)\n",
    "    \n",
    "    def forward(self, roi_features):\n",
    "        # compute classification probabilities\n",
    "        cls_score =  F.softmax(self.class_prob_head(roi_features))\n",
    "\n",
    "        # compute bounding box parameters \n",
    "        bbox_pred = self.bbox_head(roi_features)\n",
    "        \n",
    "        return (cls_score,bbox_pred)\n",
    "    \n",
    "    def init_weights(self, pretrained_model_file):\n",
    "        with open(pretrained_model_file, 'rb') as model_pickle_file:\n",
    "            fb_model = pickle.load(model_pickle_file)\n",
    "            # Model has two keys- config and blobs\n",
    "            fb_model = fb_model['blobs']        \n",
    " \n",
    "            self.class_prob_head.weight.data = torch.FloatTensor(fb_model['cls_score_w'])\n",
    "            self.class_prob_head.bias.data = torch.FloatTensor(fb_model['cls_score_b'])\n",
    "\n",
    "            self.bbox_head.weight.data = torch.FloatTensor(fb_model['bbox_pred_w'])\n",
    "            self.bbox_head.bias.data = torch.FloatTensor(fb_model['bbox_pred_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "    def __init__(self,\n",
    "                 backbone_architecture,\n",
    "                 pretrained_model_file, \n",
    "                 resnet_feature_extraction_layers, \n",
    "                 feature_extractor_output_channels,\n",
    "                 rpn_conv_output_channels,\n",
    "                 number_of_anchors,\n",
    "                 conv_head_layers,\n",
    "                 roi_height,\n",
    "                 roi_width,\n",
    "                 roi_spatial_scale,\n",
    "                 roi_sampling_ratio,\n",
    "                 roi_feature_size,\n",
    "                 N_classes):\n",
    "        super(Detector, self).__init__() \n",
    "        \n",
    "        self.backbone_architecture               = backbone_architecture\n",
    "        self.pretrained_model_file               = pretrained_model_file     \n",
    "        self.resnet_feature_extraction_layers    = resnet_feature_extraction_layers     \n",
    "        self.feature_extractor_output_channels   = feature_extractor_output_channels    \n",
    "        self.rpn_conv_output_channels            = rpn_conv_output_channels    \n",
    "        self.number_of_anchors                   = number_of_anchors    \n",
    "        self.conv_head_layers                    = conv_head_layers    \n",
    "        self.roi_height                          = roi_height    \n",
    "        self.roi_width                           = roi_width    \n",
    "        self.roi_spatial_scale                   = roi_spatial_scale    \n",
    "        self.roi_sampling_ratio                  = roi_sampling_ratio    \n",
    "        self.roi_feature_size                    = roi_feature_size    \n",
    "        self.N_classes                           = N_classes\n",
    "\n",
    "        self.resnet_model = ResnetModel(backbone_architecture = backbone_architecture,\n",
    "                                        pretrained_model_file= pretrained_model_file,\n",
    "                                        resnet_feature_extraction_layers= resnet_feature_extraction_layers)\n",
    "        \n",
    "        self.rpn = RegionProposalNetwork(pretrained_model_file= pretrained_model_file,\n",
    "                                         feature_extractor_output_channels= feature_extractor_output_channels,\n",
    "                                         rpn_conv_output_channels= rpn_conv_output_channels,\n",
    "                                         number_of_anchors= number_of_anchors)\n",
    "        self.proposal_generator = GenerateProposals(train=False)\n",
    "        \n",
    "        self.roi_pooling = ROI_Pooling(roi_height, roi_width, roi_spatial_scale, roi_sampling_ratio, conv_head_layers, self.resnet_model.resnet_model)\n",
    "        \n",
    "        self.rcnn = RCNN(pretrained_model_file, roi_feature_size, N_classes)\n",
    "       \n",
    "    def forward(self, image, scaling_factor=None):\n",
    "        h,w = image.size(2), image.size(3)\n",
    "\n",
    "        img_features = self.resnet_model(image)\n",
    "        \n",
    "        print 'Image features size ' + str(img_features.shape)\n",
    "        \n",
    "        print 'Number of channels should match ' + str(self.feature_extractor_output_channels)\n",
    "        \n",
    "        rpn_cls_prob, rpn_bbox_pred = self.rpn(img_features)\n",
    "        \n",
    "        print 'RPNs class ' + str(rpn_cls_prob.shape)\n",
    "        print 'RPNs box ' + str(rpn_bbox_pred.shape) \n",
    "        \n",
    "        rois, rpn_roi_probs = self.proposal_generator(rpn_cls_prob, rpn_bbox_pred, h, w, scaling_factor)\n",
    "        \n",
    "        print 'ROIS ' + str(rois.shape)\n",
    "        \n",
    "        roi_features = self.roi_pooling(img_features, rois)\n",
    "        \n",
    "        print 'After ROI Pooing ' + str(roi_features.shape)\n",
    "        print 'This should match ' + str(self.roi_feature_size) \n",
    "        \n",
    "        cls_score,bbox_pred = self.rcnn(roi_features)\n",
    "        \n",
    "        print 'Final class ' + str(cls_score.shape) \n",
    "        print 'Final box ' + str(bbox_pred.shape) \n",
    "        \n",
    "        return (cls_score,bbox_pred,rois,img_features,rpn_cls_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model =  Detector(backbone_architecture='resnet50',\n",
    "                 pretrained_model_file = R_50_C4_PATH, \n",
    "                 resnet_feature_extraction_layers = ['conv1','bn1','relu','maxpool','layer1','layer2','layer3'], \n",
    "                 feature_extractor_output_channels = 1024,\n",
    "                 rpn_conv_output_channels = 1024,\n",
    "                 number_of_anchors = 15,\n",
    "                 conv_head_layers = ['layer4','avgpool'],\n",
    "                 roi_height = 14,\n",
    "                 roi_width = 14,\n",
    "                 roi_spatial_scale = 0.0625,\n",
    "                 roi_sampling_ratio = 0,\n",
    "                 roi_feature_size = 2048,\n",
    "                 N_classes = 81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(sample):\n",
    "    class_scores, bbox_deltas, rois, img_features,rpn_cls_prob = model(sample['image'],\n",
    "                                                         scaling_factor=sample['scaling_factors'].cpu().data.numpy().item())   \n",
    "    return class_scores,bbox_deltas,rois,img_features,rpn_cls_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image features size torch.Size([1, 1024, 50, 80])\n",
      "Number of channels should match 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python/2.7.12/intel/lib/python2.7/site-packages/ipykernel/__main__.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RPNs class torch.Size([1, 15, 50, 80])\n",
      "RPNs box torch.Size([1, 60, 50, 80])\n",
      "ROIS torch.Size([526, 4])\n",
      "After ROI Pooing torch.Size([526, 2048])\n",
      "This should match 2048\n",
      "Final class torch.Size([526, 81])\n",
      "Final box torch.Size([526, 324])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python/2.7.12/intel/lib/python2.7/site-packages/ipykernel/__main__.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "image_filename = 'demo/33823288584_1d21cf0a26_k.jpg'\n",
    "\n",
    "# Load image\n",
    "image = io.imread(image_filename)\n",
    "orig_im_size = image.shape\n",
    "\n",
    "# Preprocess image\n",
    "im_list, im_scales = prep_im_for_blob(image)\n",
    "\n",
    "# Build sample\n",
    "sample = {}\n",
    "sample['image'] = Variable(torch.FloatTensor(im_list[0]).permute(2,0,1).unsqueeze(0))\n",
    "sample['scaling_factors'] = Variable(torch.FloatTensor([im_scales[0]]))\n",
    "sample['original_im_size'] = Variable(torch.FloatTensor(orig_im_size))\n",
    "\n",
    "class_scores,bbox_deltas,rois,img_features,rpn_cls_prob = eval_model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# postprocess output:\n",
    "# - convert coordinates back to original image size, \n",
    "# - treshold proposals based on score,\n",
    "# - do NMS.\n",
    "scores_final, boxes_final, boxes_per_class = result_utils.postprocess_output(rois,\n",
    "                                                                            sample['scaling_factors'],\n",
    "                                                                            sample['original_im_size'],\n",
    "                                                                            class_scores,\n",
    "                                                                            bbox_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 800, 1275])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 956, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_im_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.3333\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['scaling_factors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,.,.) = \n",
       "  6.8313e-05  1.1266e-04  2.6658e-05  ...   1.1223e-04  2.2220e-04  1.0581e-04\n",
       "  2.8226e-05  3.0563e-05  1.3027e-05  ...   1.0622e-04  3.2047e-04  9.7242e-05\n",
       "  2.2221e-05  4.2975e-05  1.0456e-05  ...   6.9281e-05  1.4257e-04  1.4779e-04\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.0765e-05  5.7360e-05  2.5384e-05  ...   2.4575e-05  1.4540e-04  1.8993e-05\n",
       "  7.0114e-06  9.8308e-05  2.0899e-05  ...   3.3102e-05  8.8451e-05  2.1193e-05\n",
       "  4.4377e-05  3.3877e-04  3.7163e-05  ...   9.2786e-05  1.6923e-04  4.2703e-05\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       "  1.6783e-04  6.8259e-05  4.7707e-05  ...   2.8033e-04  2.5687e-04  3.4398e-04\n",
       "  1.9633e-04  9.8460e-05  1.3850e-04  ...   2.9458e-04  2.7348e-04  3.9299e-04\n",
       "  4.6621e-05  1.3461e-05  2.5303e-05  ...   2.1738e-04  8.5451e-05  2.3713e-04\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.3794e-04  9.4107e-04  2.8001e-03  ...   1.8474e-04  1.3092e-04  9.2709e-05\n",
       "  2.0319e-04  1.4449e-03  6.7919e-04  ...   1.4408e-04  6.7831e-05  8.6442e-05\n",
       "  1.7167e-04  3.3181e-04  1.4530e-04  ...   8.7965e-05  5.2667e-05  7.8243e-05\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "  8.0956e-04  2.2018e-04  1.2213e-04  ...   4.6539e-04  5.4052e-04  8.9122e-04\n",
       "  6.6662e-04  6.0673e-05  5.1507e-05  ...   8.6973e-05  1.3606e-04  3.2596e-04\n",
       "  4.9557e-04  7.3016e-05  4.0471e-05  ...   3.3887e-04  2.2742e-04  3.4249e-04\n",
       "                 ...                   ⋱                   ...                \n",
       "  4.5519e-03  3.3817e-03  3.1455e-03  ...   2.9062e-05  5.0225e-05  5.8034e-05\n",
       "  3.2751e-04  3.6247e-04  1.4406e-04  ...   5.8054e-05  6.6735e-05  9.9485e-05\n",
       "  1.1983e-03  7.9205e-04  3.0270e-04  ...   2.1880e-04  1.7384e-04  2.3175e-04\n",
       "   ...\n",
       "\n",
       "(0 ,12,.,.) = \n",
       "  9.0111e-05  6.7750e-05  3.0728e-05  ...   2.3055e-04  1.8753e-04  3.0923e-04\n",
       "  1.6234e-05  6.1184e-06  6.9253e-06  ...   3.2962e-05  2.2763e-05  6.2986e-05\n",
       "  3.4414e-05  1.2875e-05  9.1130e-06  ...   2.9927e-04  1.3462e-04  2.5323e-04\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.5624e-05  1.3624e-03  1.0197e-02  ...   6.3999e-03  4.1286e-03  2.3624e-04\n",
       "  1.6541e-05  9.8817e-04  2.6447e-03  ...   4.3846e-03  1.3214e-03  2.4873e-04\n",
       "  1.3913e-04  2.8734e-03  3.9457e-03  ...   1.6685e-03  1.0619e-03  2.6609e-04\n",
       "\n",
       "(0 ,13,.,.) = \n",
       "  1.2086e-03  3.9536e-04  3.6586e-04  ...   1.0870e-03  6.4207e-04  1.5561e-03\n",
       "  1.5217e-04  1.7846e-05  4.9763e-05  ...   2.1168e-04  8.8932e-05  3.5456e-04\n",
       "  1.5090e-04  1.8478e-05  3.1760e-05  ...   4.9096e-04  2.0074e-04  6.4820e-04\n",
       "                 ...                   ⋱                   ...                \n",
       "  3.8167e-03  3.5279e-02  1.2393e-01  ...   1.0003e-03  2.5748e-04  3.3118e-04\n",
       "  5.1294e-03  5.3715e-02  1.3680e-01  ...   9.7660e-04  1.3212e-04  3.9282e-04\n",
       "  7.5936e-03  4.2691e-02  8.8653e-02  ...   1.1387e-03  2.6056e-04  4.9779e-04\n",
       "\n",
       "(0 ,14,.,.) = \n",
       "  1.2291e-01  4.3031e-02  3.9319e-02  ...   5.9814e-02  8.6683e-02  7.2817e-02\n",
       "  1.0410e-01  3.4823e-02  4.4200e-02  ...   8.8710e-02  8.0105e-02  6.8141e-02\n",
       "  7.7560e-02  2.9694e-02  2.7501e-02  ...   7.6743e-02  8.6257e-02  6.8516e-02\n",
       "                 ...                   ⋱                   ...                \n",
       "  5.2695e-01  6.7741e-01  5.5414e-01  ...   2.8069e-02  2.5061e-02  2.5607e-02\n",
       "  7.2707e-01  7.2355e-01  6.8000e-01  ...   4.8377e-02  4.4917e-02  4.6433e-02\n",
       "  3.0733e-01  4.2379e-01  3.9938e-01  ...   4.5694e-02  4.2917e-02  4.9142e-02\n",
       "[torch.FloatTensor of size 1x15x50x80]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_cls_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rois"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
