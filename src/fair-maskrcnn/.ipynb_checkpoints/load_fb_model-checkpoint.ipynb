{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from generate_proposals import GenerateProposals\n",
    "from roi_align import RoIAlignFunction, preprocess_rois\n",
    "import utils.vis as vis_utils\n",
    "import utils.result_utils as result_utils\n",
    "import skimage.io as io\n",
    "from utils.blob import prep_im_for_blob\n",
    "import utils.dummy_datasets as dummy_datasets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_101_FPN_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/35861858-R-101-FPN/model_final.pkl\"\n",
    "X_152_32x8d_FPN_IN5k_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/37129812-X-152-32x8d-FPN-IN5k/model_final.pkl\"\n",
    "R_101_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/R-101/R-101.pkl\"\n",
    "X_152_32x8d_IN5k_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/X-152-32x8d-IN5k/X-152-32x8d-IN5k.pkl\"\n",
    "\n",
    "R_50_C4_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/tmp/model_final.pkl\"\n",
    "R_50_PATH = \"/home/at3577/driveME/src/fair-maskrcnn/models/tmp/R-50.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResnetModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_file, resnet_feature_extraction_layers=['conv1','bn1','relu','maxpool','layer1','layer2','layer3']):\n",
    "        super(ResnetModel, self).__init__()\n",
    "        \n",
    "        self.resnet_model = eval('models.resnet101()') # construct ResNet model (maybe not very safe :) \n",
    "\n",
    "        # swap stride (2,2) and (1,1) in first layers (PyTorch ResNet is slightly different to caffe2 ResNet)\n",
    "        # this is required for compatibility with caffe2 models\n",
    "        self.resnet_model.layer2[0].conv1.stride=(2,2)\n",
    "        self.resnet_model.layer2[0].conv2.stride=(1,1)\n",
    "        self.resnet_model.layer3[0].conv1.stride=(2,2)\n",
    "        self.resnet_model.layer3[0].conv2.stride=(1,1)\n",
    "        self.resnet_model.layer4[0].conv1.stride=(2,2)\n",
    "        self.resnet_model.layer4[0].conv2.stride=(1,1)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        # All except the last layer are used as feature extractor... Last layer is for ROI pooling\n",
    "        self.model = torch.nn.Sequential(*[getattr(self.resnet_model, layer) for layer in resnet_feature_extraction_layers])\n",
    "        self.model.eval()\n",
    "        \n",
    "    def forward(self, image):\n",
    "        return self.model(image)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        with open(pretrained_model_file, 'rb') as model_pickle_file:\n",
    "            fb_model = pickle.load(model_pickle_file)\n",
    "            # Model has two keys- config and blobs\n",
    "            fb_model = fb_model['blobs']\n",
    "        \n",
    "        model_dict = self.resnet_model.state_dict()\n",
    "        \n",
    "        for key in model_dict.keys():\n",
    "            # skip running mean/std and fc weights\n",
    "            # I am not sure what running is but fc is the last fuly connected layer of resnet.. so fb model doesnt have it\n",
    "            if 'running' in key or 'fc' in key:\n",
    "                continue\n",
    "            \n",
    "            fb_key = self.convert_key_to_fb_format(key.split('.'))\n",
    "           \n",
    "            assert model_dict[key].size()==torch.FloatTensor(fb_model[fb_key]).size()\n",
    "            \n",
    "            if key=='conv1.weight': # convert from BGR to RGB                \n",
    "                model_dict[key]=torch.FloatTensor(fb_model[fb_key][:,(2, 1, 0),:,:])\n",
    "            else:\n",
    "                model_dict[key]=torch.FloatTensor(fb_model[fb_key])\n",
    "        \n",
    "        # update model\n",
    "        self.resnet_model.load_state_dict(model_dict)\n",
    "\n",
    "    def convert_key_to_fb_format(self, terms, i=0, parsed=''):\n",
    "        # Convert PyTorch ResNet weight names to caffe2 weight names\n",
    "        if i==0:\n",
    "            if terms[i]=='conv1':\n",
    "                parsed='conv1'\n",
    "            elif terms[i]=='bn1':\n",
    "                parsed='res_conv1'\n",
    "            elif terms[i].startswith('layer'):\n",
    "                parsed='res'+str(int(terms[i][-1])+1)\n",
    "        else:\n",
    "            if terms[i]=='weight' and (terms[i-1].startswith('conv') or terms[i-1]=='0'):\n",
    "                parsed+='_w'\n",
    "            elif terms[i]=='weight' and (terms[i-1].startswith('bn') or terms[i-1]=='1'):\n",
    "                parsed+='_bn_s'\n",
    "            elif terms[i]=='bias' and (terms[i-1].startswith('bn') or terms[i-1]=='1'):\n",
    "                parsed+='_bn_b'\n",
    "            elif terms[i-1].startswith('layer'):\n",
    "                parsed+='_'+terms[i]\n",
    "            elif terms[i].startswith('conv') or terms[i].startswith('bn'):\n",
    "                parsed+='_branch2'+chr(96+int(terms[i][-1]))\n",
    "            elif terms[i]=='downsample':\n",
    "                parsed+='_branch1'\n",
    "        # increase counter\n",
    "        i+=1\n",
    "        # do recursion\n",
    "        if i==len(terms):\n",
    "            return parsed\n",
    "        return self.convert_key_to_fb_format(terms,i,parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, pretrained_model_file, feature_extractor_output_channels, rpn_conv_output_channels, number_of_anchors):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        \n",
    "        #RPN is used propose regions with probability of foreground/background.. i.e just tell if object is present\n",
    "        # It has 3 parts:\n",
    "        # 1) 3x3 conv with 512/1024 channels\n",
    "        # 2) 1x1 conv with 2k channels (for each anchor box we predict foreground/background)\n",
    "        # 3) 1x1 conv with 4k channels (for each anchor box we predict delta of boxes)\n",
    "        \n",
    "        self.conv_rpn = torch.nn.Conv2d(in_channels=feature_extractor_output_channels,\n",
    "                                        out_channels=rpn_conv_output_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "        self.rpn_cls_prob = torch.nn.Conv2d(in_channels=rpn_conv_output_channels,\n",
    "                                            out_channels=number_of_anchors,\n",
    "                                            kernel_size=1,\n",
    "                                            stride=1,\n",
    "                                            padding=0)\n",
    "        self.rpn_bbox_pred = torch.nn.Conv2d(in_channels=rpn_conv_output_channels,\n",
    "                                             out_channels=4*number_of_anchors,\n",
    "                                             kernel_size=1,\n",
    "                                             stride=1,\n",
    "                                             padding=0)\n",
    "        \n",
    "    def forward(self, anchor_features):\n",
    "        # image features shape should be (N,Cin,H,W)  \n",
    "        conv_anchor_features = F.relu(self.conv_rpn(anchor_features))\n",
    "        anchor_cls_prob =  F.softmax(self.rpn_cls_prob(conv_anchor_features))\n",
    "        anchor_box_pred =  self.rpn_bbox_pred(conv_anchor_features)\n",
    "        return anchor_cls_prob, anchor_box_pred\n",
    "\n",
    "    def init_weights(self):\n",
    "        with open(pretrained_model_file, 'rb') as model_pickle_file:\n",
    "            fb_model = pickle.load(model_pickle_file)\n",
    "            # Model has two keys- config and blobs\n",
    "            fb_model = fb_model['blobs']\n",
    "        \n",
    "            self.rpn.conv_rpn.weight.data = torch.FloatTensor(fb_model['conv_rpn_w'])\n",
    "            self.rpn.conv_rpn.bias.data = torch.FloatTensor(fb_model['conv_rpn_b'])\n",
    "            self.rpn.rpn_cls_prob.weight.data = torch.FloatTensor(fb_model['rpn_cls_logits_w'])\n",
    "            self.rpn.rpn_cls_prob.bias.data = torch.FloatTensor(fb_model['rpn_cls_logits_b'])\n",
    "            self.rpn.rpn_bbox_pred.weight.data = torch.FloatTensor(fb_model['rpn_bbox_pred_w'])\n",
    "            self.rpn.rpn_bbox_pred.bias.data = torch.FloatTensor(fb_model['rpn_bbox_pred_b'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ROI_POOLING\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, roi_feature_channels, N_classes):\n",
    "        super(RCNN, self).__init__()\n",
    "        # What will be the size of roi_feature_channels??\n",
    "        self.bbox_head=torch.nn.Linear(roi_feature_channels, 4*N_classes)\n",
    "        self.class_prob_head=torch.nn.Linear(roi_feature_channels, N_classes)\n",
    "    \n",
    "    def forward(self, roi_features):\n",
    "        # compute classification probabilities\n",
    "        cls_score =  F.softmax(self.class_prob_head(roi_features))\n",
    "\n",
    "        # compute bounding box parameters \n",
    "        bbox_pred = self.bbox_head(roi_features)\n",
    "        \n",
    "        return (cls_score,bbox_pred)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "    def __init__(self, \n",
    "                 pretrained_model_file, \n",
    "                 resnet_feature_extraction_layers, \n",
    "                 feature_extractor_output_channels,\n",
    "                 rpn_conv_output_channels,\n",
    "                 number_of_anchors):\n",
    "        super(Detector, self).__init__() \n",
    "        \n",
    "        self.resnet_model = ResnetModel(pretrained_model_file= pretrained_model_file,\n",
    "                                        resnet_feature_extraction_layers= resnet_feature_extraction_layers)\n",
    "        self.rpn = RegionProposalNetwork(pretrained_model_file= pretrained_model_file,\n",
    "                                         feature_extractor_output_channels= feature_extractor_output_channels,\n",
    "                                         rpn_conv_output_channels= rpn_conv_output_channels,\n",
    "                                         number_of_anchors= number_of_anchors)\n",
    "        self.proposal_generator = GenerateProposals(train=False)\n",
    "    \n",
    "    def forward(self, image, scaling_factor=None):\n",
    "        h,w = image.size(2), image.size(3)\n",
    "\n",
    "        img_features = self.resnet_model(image)\n",
    "        \n",
    "        rpn_cls_prob, rpn_bbox_pred = self.rpn(img_features)\n",
    "        rois, rpn_roi_probs = self.proposal_generator(rpn_cls_prob, rpn_bbox_pred, h, w, scaling_factor)\n",
    "        \n",
    "        return (rpn_cls_prob,rpn_bbox_pred,rois,img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=  detector(arch=arch,\n",
    "                 detector_pkl_file=pretrained_model_file,\n",
    "                 use_rpn_head = use_rpn_head,\n",
    "                 use_mask_head = use_mask_head)\n",
    "\n",
    "def eval_model(sample):\n",
    "    class_scores,bbox_deltas,rois,img_features=model(sample['image'],\n",
    "                                                     sample['proposal_coords'],\n",
    "                                                     scaling_factor=sample['scaling_factors'].cpu().data.numpy().item())   \n",
    "    return class_scores,bbox_deltas,rois,img_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_filename = 'demo/33823288584_1d21cf0a26_k.jpg'\n",
    "\n",
    "# Load image\n",
    "image = io.imread(image_filename)\n",
    "orig_im_size = image.shape\n",
    "\n",
    "# Preprocess image\n",
    "im_list, im_scales = prep_im_for_blob(image)\n",
    "\n",
    "# Build sample\n",
    "sample = {}\n",
    "sample['image'] = torch.FloatTensor(im_list[0]).permute(2,0,1).unsqueeze(0)\n",
    "sample['scaling_factors'] = torch.FloatTensor([im_scales[0]])\n",
    "sample['original_im_size'] = torch.FloatTensor(orig_im_size)\n",
    "sample['proposal_coords']=torch.FloatTensor([-1]) # dummy value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
